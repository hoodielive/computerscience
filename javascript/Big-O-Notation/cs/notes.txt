Big 0 - worse case scenario 

We say that an algorithm is O(f(n)) if the number of simple operations the computer has to do is eventually less than a constant
times f(n), as n increases. 
+ f(n) could be linear (f(n) = n) // lambda calculus? 
+ f(n) could be quadratic (f(n) = n2)
+ f(n) could be constant (f(n) = 1)
+ f(n) could be something different entirely

Big O Shorthands
1. Arithmetic operations are constant. 
2. Variable assignment is constant.
3. Accessing elements in an array (by index) or object (by key) is constant.
4. In a loop, the time complexity is the length of the loop times the complexity of whatever happens inside the loop.

function logAtLeast5(n)  {
  for (var i = 1; i <= Math.max(5, n); i++) {
    console.log(i); 
  }
}
function logAtMost5(n)  {
  for (var i = 1; i <= Math.min(5, n); i++) {
    console.log(i); 
  }
}

+ time ( exec time or execution time) 
+ space (memory consumed) 

Big-O primer : 
	- Big-O-Notation measures the worst-case complexity of an algorithm. 
	- n represents the number of inputs 
	- The question is poses is, "What will happen as n approaches infinity?" 
	- When you implement an algorithm, Big-O-Notation is important because it tells you how efficient an algorithm
	  is. 
Again -> O(1) is constant time - does not change with respect to input space. 
Example: 
	Accessing an item in the array by its index. 

Again -> O(n) is linear time and applies to algorithms that must do n operations in the worst-case scenario.


